{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cell\n",
    "print(\"Hello from AgriSense!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b15122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python version\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d7a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"‚úÖ Running on Google Colab\")\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    print(\"‚úÖ Running locally\")\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fe8517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple calculation\n",
    "result = 2 + 2\n",
    "print(f\"2 + 2 = {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679a3735",
   "metadata": {},
   "source": [
    "## üéÆ Google Colab T4 GPU Capabilities Check\n",
    "\n",
    "Test your T4 GPU specs and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c2efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and details\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéÆ GPU AVAILABILITY CHECK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"\\n‚úÖ CUDA Available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    # GPU count\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"‚úÖ Number of GPUs: {gpu_count}\")\n",
    "    \n",
    "    # Current GPU\n",
    "    current_gpu = torch.cuda.current_device()\n",
    "    print(f\"‚úÖ Current GPU Index: {current_gpu}\")\n",
    "    \n",
    "    # GPU name\n",
    "    gpu_name = torch.cuda.get_device_name(current_gpu)\n",
    "    print(f\"‚úÖ GPU Name: {gpu_name}\")\n",
    "    \n",
    "    # CUDA version\n",
    "    cuda_version = torch.version.cuda\n",
    "    print(f\"‚úÖ CUDA Version: {cuda_version}\")\n",
    "    \n",
    "    # PyTorch version\n",
    "    print(f\"‚úÖ PyTorch Version: {torch.__version__}\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU available - check Colab runtime settings\")\n",
    "    print(\"   Go to: Runtime > Change runtime type > Hardware accelerator > T4 GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446fb502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed T4 GPU Specifications\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìä NVIDIA T4 GPU SPECIFICATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Memory information\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "    reserved_memory = torch.cuda.memory_reserved(0)\n",
    "    allocated_memory = torch.cuda.memory_allocated(0)\n",
    "    free_memory = reserved_memory - allocated_memory\n",
    "    \n",
    "    print(f\"\\nüíæ Memory Information:\")\n",
    "    print(f\"   Total Memory: {total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"   Reserved Memory: {reserved_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"   Allocated Memory: {allocated_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"   Free Memory: {(total_memory - allocated_memory) / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Compute capability\n",
    "    major, minor = torch.cuda.get_device_capability(0)\n",
    "    print(f\"\\nüî¢ Compute Capability: {major}.{minor}\")\n",
    "    \n",
    "    # Multi-processor count\n",
    "    multi_processor_count = torch.cuda.get_device_properties(0).multi_processor_count\n",
    "    print(f\"üßÆ Multi-Processor Count: {multi_processor_count}\")\n",
    "    \n",
    "    # Maximum threads per block\n",
    "    max_threads_per_block = torch.cuda.get_device_properties(0).max_threads_per_block\n",
    "    print(f\"üßµ Max Threads per Block: {max_threads_per_block}\")\n",
    "    \n",
    "    # Maximum threads per multi-processor\n",
    "    max_threads_per_mp = torch.cuda.get_device_properties(0).max_threads_per_multi_processor\n",
    "    print(f\"üîÑ Max Threads per Multi-Processor: {max_threads_per_mp}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Your T4 GPU is ready for ML training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e73ba53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Performance Benchmark\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚ö° GPU PERFORMANCE BENCHMARK\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    # Matrix multiplication benchmark\n",
    "    matrix_size = 5000\n",
    "    print(f\"\\nüß™ Testing matrix multiplication ({matrix_size}x{matrix_size})...\")\n",
    "    \n",
    "    # CPU benchmark\n",
    "    print(\"\\nüìä CPU Performance:\")\n",
    "    cpu_tensor_a = torch.randn(matrix_size, matrix_size)\n",
    "    cpu_tensor_b = torch.randn(matrix_size, matrix_size)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    cpu_result = torch.matmul(cpu_tensor_a, cpu_tensor_b)\n",
    "    cpu_time = time.time() - start_time\n",
    "    print(f\"   Time: {cpu_time:.4f} seconds\")\n",
    "    \n",
    "    # GPU benchmark\n",
    "    print(\"\\nüéÆ GPU (T4) Performance:\")\n",
    "    gpu_tensor_a = torch.randn(matrix_size, matrix_size).cuda()\n",
    "    gpu_tensor_b = torch.randn(matrix_size, matrix_size).cuda()\n",
    "    \n",
    "    # Warm-up\n",
    "    _ = torch.matmul(gpu_tensor_a, gpu_tensor_b)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    gpu_result = torch.matmul(gpu_tensor_a, gpu_tensor_b)\n",
    "    torch.cuda.synchronize()\n",
    "    gpu_time = time.time() - start_time\n",
    "    print(f\"   Time: {gpu_time:.4f} seconds\")\n",
    "    \n",
    "    # Speed comparison\n",
    "    speedup = cpu_time / gpu_time\n",
    "    print(f\"\\nüöÄ GPU Speedup: {speedup:.2f}x faster than CPU\")\n",
    "    print(f\"   CPU took: {cpu_time:.4f}s\")\n",
    "    print(f\"   GPU took: {gpu_time:.4f}s\")\n",
    "    print(f\"   Time saved: {cpu_time - gpu_time:.4f}s\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU not available for benchmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check nvidia-smi (system info)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üñ•Ô∏è NVIDIA-SMI OUTPUT\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        print(result.stdout)\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö†Ô∏è nvidia-smi not found in PATH\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error running nvidia-smi: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82359b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Framework Compatibility Test\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ü§ñ ML FRAMEWORK COMPATIBILITY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Test TensorFlow (if available)\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        print(f\"\\n‚úÖ TensorFlow: {tf.__version__}\")\n",
    "        print(f\"   GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "        if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "            print(f\"   GPU Device: {tf.config.list_physical_devices('GPU')[0].name}\")\n",
    "    except ImportError:\n",
    "        print(\"\\n‚ö†Ô∏è TensorFlow not installed\")\n",
    "    \n",
    "    # PyTorch (already imported)\n",
    "    print(f\"\\n‚úÖ PyTorch: {torch.__version__}\")\n",
    "    print(f\"   CUDA Available: {torch.cuda.is_available()}\")\n",
    "    print(f\"   cuDNN Available: {torch.backends.cudnn.is_available()}\")\n",
    "    print(f\"   cuDNN Version: {torch.backends.cudnn.version()}\")\n",
    "    \n",
    "    # Test a simple neural network\n",
    "    print(\"\\nüß† Testing Simple Neural Network on GPU...\")\n",
    "    \n",
    "    # Create a simple model\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(100, 256),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(256, 128),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(128, 10)\n",
    "    ).cuda()\n",
    "    \n",
    "    # Test forward pass\n",
    "    input_data = torch.randn(32, 100).cuda()\n",
    "    output = model(input_data)\n",
    "    \n",
    "    print(f\"   ‚úÖ Model successfully running on GPU\")\n",
    "    print(f\"   Input shape: {input_data.shape}\")\n",
    "    print(f\"   Output shape: {output.shape}\")\n",
    "    print(f\"   Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Memory usage after model\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**2\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**2\n",
    "    print(f\"\\nüíæ GPU Memory After Model Load:\")\n",
    "    print(f\"   Allocated: {allocated:.2f} MB\")\n",
    "    print(f\"   Reserved: {reserved:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea97402",
   "metadata": {},
   "source": [
    "## üìä Summary\n",
    "\n",
    "**Expected T4 GPU Specifications:**\n",
    "- **Memory**: 16 GB GDDR6\n",
    "- **CUDA Cores**: 2,560\n",
    "- **Tensor Cores**: 320\n",
    "- **Memory Bandwidth**: 320 GB/s\n",
    "- **Compute Capability**: 7.5\n",
    "- **FP32 Performance**: 8.1 TFLOPS\n",
    "- **Tensor Performance**: 65 TFLOPS (FP16)\n",
    "\n",
    "**Perfect for AgriSense ML tasks:**\n",
    "- ‚úÖ Crop recommendation model training\n",
    "- ‚úÖ Plant disease detection (CNN)\n",
    "- ‚úÖ Weed identification (Vision AI)\n",
    "- ‚úÖ Time-series predictions\n",
    "- ‚úÖ Hybrid AI model inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
