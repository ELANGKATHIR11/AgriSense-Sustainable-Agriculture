# =============================================================================
# AgriSense ML Training Configuration
# =============================================================================
# 
# This file contains hyperparameters and training schedules for all model groups.
# Modify values here instead of in code for easier experiment tracking.
# =============================================================================

# Global settings
global:
  seed: 42
  log_level: INFO
  experiment_tracking:
    enabled: true
    backend: mlflow  # mlflow, wandb, or none
    project_name: agrisense-ml-v2
  
  # Data paths
  data_dir: ../data
  models_dir: ../models
  
  # Hardware
  device: auto  # auto, cuda, cpu, mps
  num_workers: 4
  mixed_precision: true

# =============================================================================
# GROUP A: TABULAR MODELS (CatBoost + TF-DF)
# =============================================================================
group_a_tabular:
  
  # Shared preprocessing
  preprocessing:
    handle_missing: median  # median, mean, drop
    categorical_encoding: native  # CatBoost handles natively
    scaling: standard  # standard, minmax, none (CatBoost doesn't need it)
    
  # SMOTE-NC configuration (for imbalanced classes)
  smote_nc:
    enabled: true
    sampling_strategy: auto  # auto, or dict of class ratios
    k_neighbors: 5
    categorical_features: [soil_type, season, state]
    
  # Mixup augmentation
  mixup:
    enabled: true
    alpha: 0.2  # Beta distribution parameter
    
  # ----- Model 1: Crop Recommendation (96-class) -----
  crop_recommendation:
    model_type: catboost_classifier
    
    catboost_params:
      iterations: 2000
      learning_rate: 0.05
      depth: 8
      l2_leaf_reg: 3.0
      
      # DART mode (Dropout Additive Regression Trees)
      boosting_type: Plain  # Plain or Ordered
      bootstrap_type: Bayesian
      bagging_temperature: 1.0
      
      # Categorical features
      cat_features: [soil_type, season, state]
      
      # Regularization
      random_strength: 1.0
      border_count: 254
      
      # Early stopping
      early_stopping_rounds: 100
      
      # GPU acceleration (if available)
      task_type: GPU  # or CPU
      devices: '0'
      
    validation:
      strategy: stratified_kfold
      n_splits: 5
      
    export:
      formats: [cbm, onnx]
      quantize_onnx: true
      
  # ----- Model 2: Yield Prediction (Regression) -----
  yield_prediction:
    model_type: catboost_regressor
    
    catboost_params:
      iterations: 1500
      learning_rate: 0.03
      depth: 6
      l2_leaf_reg: 5.0
      loss_function: RMSE
      boosting_type: Plain
      
    validation:
      strategy: kfold
      n_splits: 5
      
  # ----- TensorFlow Decision Forests (Edge Export) -----
  tfdf_edge:
    enabled: true
    model_type: gradient_boosted_trees
    
    tfdf_params:
      num_trees: 100
      max_depth: 6
      shrinkage: 0.1
      
    quantization:
      enabled: true
      representative_dataset_size: 1000
      
    export:
      format: tflite
      target: esp32_s3

# =============================================================================
# GROUP B: VISION MODELS (ConvNeXt V2 + YOLOv8-Seg)
# =============================================================================
group_b_vision:
  
  # Shared augmentation
  augmentation:
    # Standard augmentations
    horizontal_flip: 0.5
    vertical_flip: 0.3
    rotation_limit: 30
    brightness_limit: 0.2
    contrast_limit: 0.2
    
    # Copy-Paste augmentation (fixes lab bias)
    copy_paste:
      enabled: true
      max_objects: 3
      blend_mode: gaussian
      
    # Mosaic (for object detection)
    mosaic:
      enabled: true
      probability: 0.5
      
  # ----- Disease Detection (ConvNeXt V2 Nano) -----
  disease_detection:
    model_type: convnext_v2_nano
    pretrained: true
    pretrained_weights: convnextv2_nano_22k_384_ema  # From timm
    
    # Architecture
    input_size: 384
    num_classes: 38  # PlantVillage classes
    drop_path_rate: 0.1
    
    # Training
    optimizer: adamw
    learning_rate: 1.0e-4
    weight_decay: 0.05
    batch_size: 32
    epochs: 100
    
    # Scheduler
    scheduler: cosine
    warmup_epochs: 5
    min_lr: 1.0e-6
    
    # Early stopping
    patience: 15
    monitor: val_accuracy
    
    export:
      formats: [onnx, tflite]
      input_shape: [1, 3, 384, 384]
      
  # ----- Weed Segmentation (YOLOv8-Seg) -----
  weed_segmentation:
    model_type: yolov8n-seg
    pretrained: true
    
    # Architecture
    input_size: 640
    
    # Training
    batch_size: 16
    epochs: 200
    optimizer: AdamW
    learning_rate: 0.001
    weight_decay: 0.0005
    
    # Augmentation (YOLOv8 built-in)
    augment:
      hsv_h: 0.015
      hsv_s: 0.7
      hsv_v: 0.4
      degrees: 0.0
      translate: 0.1
      scale: 0.5
      shear: 0.0
      flipud: 0.5
      fliplr: 0.5
      mosaic: 1.0
      copy_paste: 0.5
      
    # Loss weights
    box: 7.5
    cls: 0.5
    dfl: 1.5
    
    export:
      formats: [onnx, pt]

# =============================================================================
# GROUP C: EDGE/NPU MODELS (1D-CNN with QAT)
# =============================================================================
group_c_edge:
  
  # Target hardware constraints
  target:
    device: esp32_s3
    max_model_size_kb: 500
    max_ram_kb: 320
    inference_time_ms: 50
    
  # ----- 1D-CNN for Crop Prediction -----
  crop_cnn1d:
    model_type: cnn1d_mobilenetv3
    
    # Architecture
    input_features: 19  # Number of sensor inputs
    num_classes: 96
    
    # MobileNetV3-Small inspired blocks
    channels: [16, 24, 40, 48]
    kernel_sizes: [3, 3, 5, 3]
    use_se: true  # Squeeze-and-Excitation
    activation: hard_swish
    
    # Training
    batch_size: 64
    epochs: 200
    optimizer: adamw
    learning_rate: 0.001
    weight_decay: 0.01
    
    # Quantization Aware Training (QAT)
    qat:
      enabled: true
      start_epoch: 50  # Start QAT after initial training
      quantize_weights: int8
      quantize_activations: int8
      
    # Post-training quantization fallback
    ptq:
      representative_samples: 500
      
    export:
      format: tflite
      quantization: int8
      target: tflite_micro
      
  # ----- Soil Health CNN -----
  soil_cnn1d:
    model_type: cnn1d_mobilenetv3
    input_features: 7  # NPK, pH, moisture, temp, conductivity
    num_classes: 5  # soil health categories
    channels: [8, 16, 24]
    kernel_sizes: [3, 3, 3]
    use_se: false  # Smaller model
    
    qat:
      enabled: true
      start_epoch: 30

# =============================================================================
# GROUP D: NLP & RAG (DistilBERT + BGE-M3)
# =============================================================================
group_d_nlp:
  
  # ----- Intent Classification (DistilBERT) -----
  intent_classifier:
    model_type: distilbert
    pretrained: distilbert-base-multilingual-cased
    
    # Architecture
    num_labels: 6  # weather, disease, soil, crop_rec, pricing, general
    max_length: 128
    
    # Training
    batch_size: 32
    epochs: 20
    optimizer: adamw
    learning_rate: 2.0e-5
    weight_decay: 0.01
    
    # Scheduler
    scheduler: linear
    warmup_ratio: 0.1
    
    export:
      formats: [onnx]
      quantize: true
      
  # ----- Embeddings (BGE-M3) -----
  embeddings:
    model_type: bge_m3
    model_name: BAAI/bge-m3
    
    # Multilingual support
    languages: [en, hi, ta, te, kn]  # English, Hindi, Tamil, Telugu, Kannada
    
    # Index configuration
    vector_dim: 1024
    index_type: faiss_hnsw
    
    export:
      format: onnx
      
  # ----- RAG Pipeline -----
  rag:
    retriever:
      top_k: 5
      similarity_threshold: 0.7
      
    reranker:
      enabled: true
      model: cross-encoder/ms-marco-MiniLM-L-6-v2
      
    generation:
      enabled: false  # Use retrieval-only for now
      # model: gpt-4o-mini (if enabled)
